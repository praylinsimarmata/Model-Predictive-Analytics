# -*- coding: utf-8 -*-
"""Proyek Pertama - Predictive Analytics.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10D1ThBi4zcYRJ0Bty4WKzqNTEfjqTZzx

# **Memprediksi Churn Nasabah Bank**

## **Import Libraries**
"""

pip install seaborn --upgrade

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
from imblearn.under_sampling import TomekLinks
import random
import xgboost as xgb
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import recall_score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import classification_report,confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import GridSearchCV

"""## **Data Preparation**

### **Melakukan Load Data dan Menampilkan Data**
"""

df = pd.read_csv('/content/BankChurners.csv')
df

y = pd.Series(np.where(df['Attrition_Flag'] == 'Attrited Customer', 1, 0), df.index)
#Menambahkan kolom y ke dalam tabel
df.insert(0,'y', pd.Series(np.where(df['Attrition_Flag'] == 'Attrited Customer', 1, 0), df.index))

#Menghapus dua kolom pertama yang tidak prediktif
df = df.drop(['Attrition_Flag','CLIENTNUM'],axis=1)
df = df.iloc[:,0:-2]

"""### **Melihat Informasi Data**"""

df.shape

df.describe()

df.info()

"""### **Mengecek Baris Kosong**"""

print('Jumlah baris kosong:')
pd.DataFrame(df.isnull().sum().reset_index()).rename( columns={0:"Total Kosong","index":"Kolom"})

"""## **Exploratory Data Analysis**

### **Plotting Data Hasil Import**
"""

#correlation plot
df.corr()

#correlation heatmap
sns.set_style('whitegrid')
plt.figure(figsize=(10,6))
sns.heatmap(df.corr(),annot=True,lw=1,robust=True,fmt='.2f',vmin=-0.5,vmax=0.5)

sns.pairplot(df, y_vars=['y'],x_vars=['Contacts_Count_12_mon',
                      'Total_Revolving_Bal','Total_Trans_Ct','Total_Ct_Chng_Q4_Q1'],kind='scatter')

fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(18,8))

sns.boxplot(x='y',y='Total_Ct_Chng_Q4_Q1', data=df, ax=axs[0,0])
sns.boxplot(x='y',y='Total_Trans_Ct', data=df, ax=axs[0,1])
sns.boxplot(x='y',y='Total_Revolving_Bal', data=df, ax=axs[0,2])

sns.kdeplot(x='Total_Ct_Chng_Q4_Q1',data=df, hue='y', shade=True, ax=axs[1,0])
sns.kdeplot(x='Total_Trans_Ct',data=df, hue='y', shade=True, ax=axs[1,1])
sns.kdeplot(x='Total_Revolving_Bal',data=df, hue='y', shade=True, ax=axs[1,2])

#rata-rata jumlah transaksi tahunan untuk churner dan non-churner
df.groupby('y').agg({'Total_Trans_Amt':np.average, 'Months_on_book':np.average})

#Lifetime value of customer
p = 0.03 #profit margin, bank menghasilkan sekitar 3% per transaksi
ac = 3095 #total jumlah transaksi tahunan untuk churner
an = 4654
t = 3 #periode waktu retensi dalam tahun

# ltv = total annual transaction amount * margin per transaction * retention time period
churner_ltv = ac*p*t
nonchurner_ltv = an*p*t

print('churner ltv:',churner_ltv)
print('non churner ltv:',nonchurner_ltv)

"""## **Data Preprocessing**

### **Missing Values**

Diketahui pada dataset ini tidak ada missing values. Kode di bawah ini adalah template untuk ***impute missing values***. Untuk fitur kategori dipilih imputasi *'most frequent'* dan untuk fitur numerik dipilih *'median'*.
"""

#impute pada kolom kategorikal
from sklearn.impute import SimpleImputer

categorical_features = df.select_dtypes(include=['object']).columns
cat = SimpleImputer(strategy='most_frequent',copy=False)
cat1 = cat.fit(df[categorical_features].astype(str))
df[categorical_features] = cat1.transform(df[categorical_features])

#impute pada kolom numerik
numeric_features = df.select_dtypes(include=['int64', 'float64']).columns
num = SimpleImputer(strategy='median',copy=False)
num1 = num.fit(df[numeric_features])
df[numeric_features] = num1.transform(df[numeric_features])

"""### **Categorical Features**"""

#Membuat kolom numerikal baru dan menghapus kolom sebelumnya
categorical_features = df.select_dtypes(include=['object']).columns
X = pd.get_dummies(df[categorical_features], prefix_sep='_')
X = pd.merge(df,X,how='outer',left_index=True,right_index=True)
X = X.drop(categorical_features, axis=1)
X.head()

"""### **Feature Engineering**"""

!pip install featuretools

import featuretools as ft

"""### **Normalization / Scaling**"""

from sklearn.preprocessing import RobustScaler

scaler = RobustScaler()
index = X.columns

transformer = scaler.fit_transform(X)
X = pd.DataFrame(transformer, columns = index)
X.head()

"""### **Test-Train Split**"""

from sklearn.model_selection import train_test_split

X = X.drop(columns='y')
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 12)
x_train.shape

"""### **Over/Under Sampling**"""

y.value_counts()

#Menyeimbangkan training set
from imblearn.over_sampling import SMOTE
sm = SMOTE(random_state=12)
x_train, y_train = sm.fit_resample(x_train, y_train)
x_train.shape

"""## **Modeling and Evaluation**"""

#function untuk mengevaluasi model
def eval_model(model):
    preds = model.predict(x_test)
    probs = model.predict_proba(x_test)
    auc_prob_val = roc_auc_score(y_test, probs[:,1])
    accuracy = accuracy_score(y_test, preds)
    print('AUC Score:', auc_prob_val,'\n','Accuracy:', accuracy)
    print('\n','Confusion Matrix','\n',confusion_matrix(y_test, preds))
    print('\n','Classification Report', '\n', classification_report(y_test, preds, digits=3))

"""### **Logistic Regression**"""

reg = LogisticRegression()

#hyperparameters
solvers = ['newton-cg','lbfgs','liblinear']
penalty = ['l1','l2']
c = [100,10,1,.1,.01]

#grid search
grid = dict(solver=solvers,penalty=penalty,C=c)
cv = RepeatedStratifiedKFold(n_splits=10,n_repeats=3, random_state=1)
grid_search = GridSearchCV(estimator=reg,param_grid=grid,n_jobs=-1,cv=cv,scoring='f1',error_score=0)
fit = grid_search.fit(x_train,y_train)

eval_model(fit)

"""### **Random Forest**"""

rf = RandomForestClassifier()

#hyperparameters
n_estimators = [10,100,1000]
max_features = ['sqrt','log2']

#grid search
grid = dict(n_estimators=n_estimators,max_features=max_features)
cv = RepeatedStratifiedKFold(n_splits=5,n_repeats=3, random_state=1)
grid_search = GridSearchCV(estimator=rf,param_grid=grid,n_jobs=-1,cv=cv,scoring='f1',error_score=0)
fit = grid_search.fit(x_train,y_train)

eval_model(fit)

"""### **XG Boost**"""

xgb = XGBClassifier()

#hyperparameters
eta = [.1,.4,.7,1]
min_child_weight = [3,6,10]
max_depth = [.1,1,5,10]

#grid search
grid = dict(eta=eta,min_child_weight=min_child_weight,max_depth=max_depth)
cv = RepeatedStratifiedKFold(n_splits=5,n_repeats=3, random_state=1)
grid_search = GridSearchCV(estimator=xgb,param_grid=grid,n_jobs=-1,cv=cv,scoring='f1',error_score=0)
fit = grid_search.fit(x_train,y_train)

eval_model(fit)